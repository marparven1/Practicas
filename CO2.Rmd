---
title: "CO2"
author: "Marta Venegas Pardo"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(TSA)
library(tseries)
# install.packages('forecast',dependencies = TRUE)
library(forecast)
```


# Lectura y representación de los datos

```{r}

data(co2)
str(co2)
serie_ini<-co2
plot (serie_ini, ylab="CO2")
```


# Apartado 1 

```{r}
plot(serie_ini)
#ndiffs(serie)
#nsdiffs(serie)
```


```{r}
plot(window(serie_ini,1994,1997))
```


Observamos tendencia de los datos, ya que crecen con respecto al tiempo.
La varianza de los datos podemos considerarla constante en el tiempo, ya que no observamos que las oscilaciones crezcan cada año. 
Tambien vemos una gran dependencia de la estación, estacioariedad, ya que existe un patrón que se repite cada año.

# Paso 2. Varianza estable en el tiempo

Vamos a hacer una transformación para conseguir homogeneidad en la varianza de los datos. Buscamos en la familia de transformaciones de Box Cox

```{r}
bc=BoxCox.ar(y=serie_ini # lambda=seq(-3,3,0.01)
             )
bc$mle
bc
```
Nos sugiere una transformación con lambda = 2. El 1 está en el IC, por tanto, consideramos que los datos no necesitan transformación. 

La transformación es: $\dfrac{x^{\lambda}-1}{\lambda}$



# Paso 3. Transformación para media estable en el tiempo


Vamos a representar la FAS

```{r}
acf(serie_ini, main="FAS Datos CO2",lag.max = 50)
```



Observamos que la FAC decrece muy lentamente, se trata de un modelo integrado. Vemos mucha dependencia de la componente estacional y regular.

Las autocorrelaciones de periodo s=12 decrecen muy lentamente


```{r}
serieDiff <- diff(serie_ini,lag=1,diff=1)
acf(serieDiff, main="FAS tras una diferencia Regular", lag.max = 50)
```

Hacemos una diferencia estaciona, parece que tambien existe una dependencia de esta componente.


```{r}
serieFinal <- diff(serieDiff,lag=12,diff=1)

acf(serieFinal,lag.max = 50)
```

Podemos apreciar que esta FAC corresponde a un modelo estacionario y no se ven dependencias de la componente regular ni estacional.

Tambien puede verse representado la serie diferenciada 


```{r}
plot(serieFinal)
```

Observamos un muelle, lo que indicaría estacionariedad en los datos.

# Paso 4. Compruebo la estacionariedad

Vamos a hacer el test de Dikey-Fuller para comprobar la estacionariedad de los datos. Test de raiz unitaria que contrasta las hipótesis siguientes:

\[H_0: \text{El polinomio autoregresivo tiene una raiz unitaria}\]
\[H_1: \text{Todas las raices del polinomio autoregresivo son estacionarias (en módulo mayores que 1)}\]

```{r}
library(tseries)
adf.test(serieFinal)
```

- p-valor del contraste:  $\text{p-valor} < 0.01 < \alpha = 0.05$
- Conclusión: rechazo la hipótesis nula a favor de la alternativa, es decir, no existen evidencias diferencias significaticas para aceptar que el polinomio autoregresivo tiene una raiz unitaria.

# apartado 4 

Vamos a identificar la estructura ARIMA a través de la FAC y la FAP 

```{r}
acf(serieFinal,lag.max = 50, main ="FAC tras una diferencia regular y otra estacional")
```
En los retrardos estacionales observamos una autocorrelación en el primer retardo, podríamos pensar que la parte estacional tiene una estructura MA(1), además, a ambos lados del retardo estacional, vemos la estructura de la componente regular, solo hay una única autocorrelación a cada lado, lo cual indica que en la parte regular tambien haya una estructura MA(1).

Además, en los retardos iniciales vemos una única autocorrelación, lo que indicaría un modelo MA(1) para la parte regular.




```{r}
pacf(serieFinal,lag.max = 50, main ="FAP tras una diferencia regular y otra estacional")
```

En los retardos estacionales podemos ver dos autocorrelaciones que decrecen rápidamente, lo que avalaría la hipótesis de un MA(1) en la parte estacional.

En la parte regular, encontramos ude nuevo dos autocorrelaciones, lo que avalaría la hipótesis de un MA(1) en la parte estacional.


Modelos candidatos:

- SARIMA(0,1,1)x(0,1,1)12


## Ajuste de los modelos 

- SARIMA(0,1,1)x(0,1,1)12

```{r}
# SI PONGO EL 1 LOS DATOS SON LOS TRANSFORMADOSSSSSSS
ajuste1<-arima(serie_ini 
               ,order=c(0,1,1), # p. regular
      seasonal = list(order=c(0,1,1), period=12) # p. estacional
      )
ajuste1
confint(ajuste1)
```

No hay coeficientes significativamente nulos.

AIC=283.08

Vamos a ver si se cumple que los resíduos siguen un proceso ruido blanco.

```{r}
checkresiduals(ajuste1)
```
- p-valor del constaste: p-value =  0.2564. Puedo aceptar la incorrelación. Los resíduos siguien un ruido blanco.
- En la FAS vemos una autocorrelación que se sale, pero está dentro de la normalidad debido a que es in IC al 95%
- La representación de los resíduos tiene forma de muelle, lo que indica que las innovaciones siguen un proceso de ruido blanco

El modelo es válido.



Ajuste de autoarima


```{r}
ajuste2 <- auto.arima(serie_ini,d = 1,D = 1,stepwise = FALSE)
confint(ajuste2)
ajuste2
```



No hay coeficientes significativamente nulos. Se trata de un modelo: SARIMA(0,1,1)x(0,1,1)12, que es el que ya teníamos ajustado.
Valor del AIC=285.08





```{r}
checkresiduals(ajuste2)
```



- p-valor= 0.2564. Acepto la incorrelación de los resíduos.


## Comparación de los modelos

Ajuste 1. SARIMA(0,1,1)X(0,1,1)12 AIC=283.08
Ajuste 2. SARIMA(0,1,1)x(0,1,1)12 AIC=285.08



El modelo final es aquel con menor AIC. Los tres modelos tienen un AIC muy parecido. El modelo final es:

Ajuste 1. SARIMA(0,1,1)X(0,1,1)12 AIC=283.08


## Modelo final 

Este es mi modelo final: SARIMA(0,1,1)x(0,1,1)12

```{r}
ajusteFinal<-ajuste1
ajusteFinal
```

\(Y_t(1-L)(1-L^{12}) = (1+0.579L)(1+0.821 L^{12})\alpha_t \)


# Apartado 5

Predicción para 12 meses siguientes e IC para el precio de la leche

```{r}
pred= predict(ajusteFinal,n.ahead = 12)
pred$pred



inf<-pred$pred-qnorm(0.95)*pred$se
sup<-pred$pred+qnorm(0.95)*pred$se

cbind.data.frame(pred,inf,sup)

```

# Apartado 6 

```{r}
dec=decompose(serie_ini,type="additive")
serie_dec= serie_ini-dec$seasonal # serie desestacionalizada
```

Ahora ajustamos a una recta la serie desestracionalizada



```{r}
time = time(serie_dec)
tendencia = lm(serie_dec~time)
summary(tendencia)
```




```{r}
cc=1:12/12
cc=time[132]+cc
cc=data.frame(time=cc)
# cc # M móviles centradas
```

```{r}
# prediccion de la tendencia 
pred_tendencia = predict(tendencia,cc)
# dec$figure # IVE
season = dec$figure
p2 = ts(pred_tendencia+season, freq=12,start=c(2005,1))
p2
```





# Apartado 7 

```{r}
serie_suavizado <- HoltWinters(serie_ini)
p3 <- predict(serie_suavizado,n.ahead=12)
p3
```

# Apartado 8


```{r}
# y = x^lambda -> datos = y^(1/lambda) 
p1<-pred$pred

plot(serie_ini,xlim=c(1994,2006),lwd=2)
lines(p1,col="red",lwd=2)
lines(p2,col="blue",lwd=2)
lines(p3,col="green",lwd=2)
```


Ambas son muy parecidas.



